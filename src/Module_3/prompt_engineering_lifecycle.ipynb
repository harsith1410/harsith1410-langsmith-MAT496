{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T17:27:30.035936Z",
     "start_time": "2025-10-11T17:27:30.011926Z"
    }
   },
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T17:27:58.787185Z",
     "start_time": "2025-10-11T17:27:31.655862Z"
    }
   },
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith with the @traceable decorator in Python, first ensure the LANGSMITH_TRACING environment variable is set to 'true' and the LANGSMITH_API_KEY is provided. Then, simply decorate any function you want to trace with @traceable. Remember to use the await keyword if you're wrapping synchronous functions to log traces correctly.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T17:30:30.372026Z",
     "start_time": "2025-10-11T17:30:28.550425Z"
    }
   },
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"What is DRS in Formula 1?\",\n",
    "        \"\"\"F1 Technical Regulations | Aerodynamics\\n\\nArticle 3.10: Aerodynamic Components\\n\\n3.10.8 Drag Reduction System (DRS)\\n\\nThe Drag Reduction System (DRS) is a driver-operated device that allows for an adjustable rear wing. The system is designed to aid overtaking. When activated, it opens a flap in the rear wing, which reduces aerodynamic drag and allows the car to achieve a higher top speed. The use of DRS is subject to specific conditions: it can only be activated in designated 'DRS zones' on the track during the race, and only when a car is detected to be less than one second behind the car in front at a specific detection point. The system is disabled immediately when the driver brakes. In practice sessions and qualifying, drivers may use it within the designated zones regardless of the gap to the car ahead.\\n\\nHome | Rules | Sporting Regulations | Technical Regulations | Contact\\n\\nF1 Explained: Overtaking Aids\\nOne of the key innovations in modern Formula 1 is the Drag Reduction System. It was introduced in 2011 to promote more overtaking opportunities. The primary function is to reduce the 'dirty air' effect and give the following car a temporary speed advantage on the straights. Without it, passing can be notoriously difficult on certain circuits.\"\"\",\n",
    "        \"The Drag Reduction System (DRS) is a tool that helps with overtaking in F1. When activated by the driver in specific 'DRS zones' on the track, a flap on the rear wing opens. This reduces aerodynamic drag, giving the car a temporary speed boost. To use it in a race, a car must be less than one second behind another car at a designated detection point.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How does the F1 points system work?\",\n",
    "        \"\"\"FIA Formula 1 Sporting Regulations - Article 6: Championship Points\\n\\n6.4 Points for the Championship\\nPoints are awarded to classified drivers and their teams for each Grand Prix according to the following scale:\\n\\n- 1st Place: 25 points\\n- 2nd Place: 18 points\\n- 3rd Place: 15 points\\n- 4th Place: 12 points\\n- 5th Place: 10 points\\n- 6th Place: 8 points\\n- 7th Place: 6 points\\n- 8th Place: 4 points\\n- 9th Place: 2 points\\n- 10th Place: 1 point\\n\\n6.5 Fastest Lap Point\\nAn additional point will be awarded to the driver who achieves the fastest valid lap time during the Grand Prix, provided they are also classified in the top 10 finishers. This point is also awarded to their team. If the driver with the fastest lap finishes outside the top 10, no fastest lap point is awarded for that race.\\n\\nRace Weekend Guide | Points and Standings | Sprint Race Rules\\n\\nUnderstanding the stakes in F1 means understanding the points. Every position in the top ten is crucial, with the winner taking a significant haul of 25 points. This structure ensures that consistency is rewarded, but victories are paramount for a championship campaign.\"\"\",\n",
    "        \"In a Formula 1 Grand Prix, points are awarded to the top 10 finishers as follows:\\n\\n- **1st**: 25 points\\n- **2nd**: 18 points\\n- **3rd**: 15 points\\n- **4th**: 12 points\\n- **5th**: 10 points\\n- **6th**: 8 points\\n- **7th**: 6 points\\n- **8th**: 4 points\\n- **9th**: 2 points\\n- **10th**: 1 point\\n\\nAdditionally, one bonus point is given to the driver who sets the fastest lap, but only if they finish the race in the top 10.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Can you explain the different tire compounds in F1?\",\n",
    "        \"\"\"Pirelli Motorsport - The P Zero Range\\n\\nAt each Grand Prix weekend, Pirelli, the official tire supplier for Formula 1, provides three different slick tire compounds for the teams to use. These are chosen from a range of five compounds, labeled C1 (the hardest) to C5 (the softest).\\n\\nFor any given race, the three selected compounds are color-coded for easy identification:\\n\\n- **Hard Tire (White wall):** The most durable compound, designed for long stints. It offers less grip than the softer compounds but degrades much more slowly.\\n- **Medium Tire (Yellow wall):** A balanced compound that offers a good compromise between performance and durability. It is a versatile tire often used as a key part of race strategy.\\n- **Soft Tire (Red wall):** The least durable but highest-grip compound. It provides the most speed over a single lap, making it ideal for qualifying, but it wears out quickly during the race.\\n\\nIn addition to the slick tires, there are two options for wet weather conditions:\\n\\n- **Intermediate (Green wall):** For damp tracks or light rain.\\n- **Full Wet (Blue wall):** For heavy rain and standing water.\"\"\",\n",
    "        \"Pirelli provides three slick tire compounds for each F1 race, color-coded for identification:\\n\\n- **Hard (White):** The most durable tire with the least grip, used for long race stints.\\n- **Medium (Yellow):** A balanced tire offering a compromise between durability and speed.\\n- **Soft (Red):** The fastest tire with the most grip, but it wears out very quickly. Ideal for qualifying.\\n\\nThere are also two wet weather tires: the **Intermediate (Green)** for damp conditions and the **Full Wet (Blue)** for heavy rain.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"F1 questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Sample questions about F1\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['73566f62-6863-4677-9361-ed2da708530a',\n",
       "  'c471d1fe-76aa-4718-9e9b-dd8e1953958f',\n",
       "  'c84730b1-e361-45ab-a2d2-4ed2f1a85c6b'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T17:37:45.236199Z",
     "start_time": "2025-10-11T17:37:43.896630Z"
    }
   },
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langchain import hub\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
    "\n",
    "prompt = hub.pull(\"f1_simple_rag:f01615e0\")\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    # TODO: Let's use our prompt pulled from Prompt Hub instead of manually formatting here!\n",
    "\n",
    "    formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T17:37:55.098624Z",
     "start_time": "2025-10-11T17:37:46.971912Z"
    }
   },
   "source": [
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith using the `@traceable` decorator in Python, follow these steps:\\n\\n1. **Set Environment Variables**: \\n   - Ensure you set the `LANGSMITH_TRACING` environment variable to `'true'`. This enables tracing.\\n   - Set the `LANGSMITH_API_KEY` environment variable to your API key.\\n\\n2. **Install LangSmith**: Make sure you have the LangSmith SDK installed in your Python environment. You can use pip to install it if you haven't done so already.\\n\\n3. **Import the Traceable Decorator**: In your Python code, import the `traceable` decorator from the LangSmith SDK:\\n\\n   ```python\\n   from langsmith import traceable\\n   ```\\n\\n4. **Decorate Your Function**: Use the `@traceable` decorator to wrap any function you want to trace. Hereâ€™s an example:\\n\\n   ```python\\n   @traceable\\n   def my_function():\\n       # Your function code here\\n       pass\\n   ```\\n\\n5. **Call Your Decorated Function**: When you call the decorated function, it will log traces to LangSmith:\\n\\n   ```python\\n   async def main():\\n       await my_function()\\n   ```\\n\\n6. **Run Your Code**: Execute your script ensuring that the LangSmith instance is running and properly configured.\\n\\nBy following these steps, you can successfully set up tracing in your Python application using the `@traceable` decorator from LangSmith. Make sure to consult the self-hosted usage guide for any additional configuration needed based on your setup.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
