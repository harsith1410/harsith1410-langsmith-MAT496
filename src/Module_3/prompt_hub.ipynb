{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:09:55.562318Z",
     "start_time": "2025-10-11T15:09:55.488581Z"
    }
   },
   "source": [
    "from IPython.core.display import Markdown\n",
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:34:00.646264Z",
     "start_time": "2025-10-11T15:34:00.036995Z"
    }
   },
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"britain_speaker\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:34:03.586885Z",
     "start_time": "2025-10-11T15:34:03.577924Z"
    }
   },
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})\n",
    "hydrated_prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You are a pirate from 1600's and you speak only this Spanish\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you a captain yet?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:34:07.812036Z",
     "start_time": "2025-10-11T15:34:05.289079Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CPVteHURGOfRbqyLrm5TOfPdAH2Ez', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='No soy capitán aún, pero navego los mares con valentía y deseo algún día serlo. ¿Qué quieres saber sobre la vida de un pirata?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760196846, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_51db84afab', usage=CompletionUsage(completion_tokens=33, prompt_tokens=32, total_tokens=65, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "prompt = hub.pull(\"britain_speaker\",include_model=True)\n",
    "prompt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T16:26:57.210794Z",
     "start_time": "2025-10-11T16:26:55.958424Z"
    }
   },
   "source": [
    "prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '¡Aún no soy capitán, pero navego con corazón de corsario y sueño con la bandera ondeando alto en la proa!'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T16:37:45.140359Z",
     "start_time": "2025-10-11T16:37:44.490763Z"
    }
   },
   "source": "prompt = hub.pull(\"f1_prompt\")",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"topic\": \"Monza 2025 between Norris and Piastri\", \"data\":\"https://thetorquehunter.com/norris-vs-piastri-mclarens-controversial-team-orders-drama-at-monza-explained/\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt = hub.pull(\"f1_prompt\",include_model=True)\n",
    "hydrated_prompt = prompt.invoke({\"question\":\"Give me the answer in bullets\",\"topic\": \"Monza 2025 between Norris and Piastri\", \"data\":\"https://thetorquehunter.com/norris-vs-piastri-mclarens-controversial-team-orders-drama-at-monza-explained/\"})\n",
    "hydrated_prompt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hydrated_prompt['answer']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T16:43:14.645402Z",
     "start_time": "2025-10-11T16:43:14.641217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(hydrated_prompt['answer'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Balanced bullets on whether Monza 2025 Norris vs. Piastri was fair, with driver, team, and fan perspectives (and referencing the Torque Hunter explainer):\n\n- Overall verdict (fair vs. unfair):\n  - Fair-ish: Driver orders are a longstanding, practical tool in F1 to protect team goals and sponsors; Monza 2025 can be viewed as a strategic decision aimed at maximizing the team's championship outcome.\n  - Unfair-ish: Public perception of on-track fairness matters; if drivers are blocked or asked to yield, some fans and commentators see it as diminishing the competitive integrity of the sport.\n  - Bottom line: fairness is in the eye of the beholder—technical legality and strategic rationale versus on-track fairness and transparency.\n\n- What the article and coverage emphasize (the Monza drama):\n  - The Torque Hunter explainer frames it as a controversial team-order episode used to manage both drivers’ results and the team’s overall points.\n  - It highlights how the orders were communicated, how they affected race dynamics, and why fans and pundits debated the legitimacy of the moves.\n\n- Drivers’ perspectives (plausible, as discussed in media):\n  - Norris: likely places value on earning his results on track and may view heavy-handed orders as compromising his perceived fairness and personal momentum, while still recognizing the team's responsibility to maximize points.\n  - Piastri: could be portrayed as professional and pragmatic, following team instructions when asked, but fans and analysts might worry about how such orders affect his short- and long-term development.\n  - Common thread: both drivers want clear, consistent rules and communications so they know what to expect in strategic episodes like Monza.\n\n- Teams’ perspectives (McLaren and peers):\n  - McLaren’s stance (as presented in coverage): team orders are a legitimate tool to optimize the podium and constructors’ points, balancing driver talents with data-driven strategy and sponsor expectations.\n  - Alternative team view: concerns about perceived favoritism or internal tension if orders are not applied transparently or consistently across the grid.\n  - Broader F1 view: teams defend the practice as part of racing strategy, while critics push for greater transparency and rules around how and when such orders can be exercised.\n\n- Fans’ perspectives (fans on social media and in commentary):\n  - Pro-team-orders fans: appreciate strategic depth, the drama of race-day decisions, and the idea that teams fight for the best overall outcome.\n  - Anti-team-orders fans: argue that such moves reduce spectacle, undermine the idea of fair racing, and can feel like manipulating results rather than letting drivers race fairly.\n  - Neutral fans often call for better communication and clearer guidelines to understand when and why teams intervene.\n\n- Why fairness is contested (key tensions):\n  - Equal opportunity vs. optimal team strategy: does yielding one driver over another erode the principle that both drivers should have a fair chance to win?\n  - Transparency and consistency: calls for explicit, published criteria on when team orders can be used and who approves them.\n  - Impact on credibility: repeated episodes risk shaping a narrative that results are bought by strategy rather than earned on track.\n\n- What would improve perceived fairness (practical steps):\n  - Clear, pre-agreed guidelines for when team orders can be executed, with driver consent documented.\n  - Transparent team communication about orders (timing, rationale) to fans and the media.\n  - Independent review or FIA-approved framework for assessing controversial episodes after races to maintain consistency across teams.\n\n- Bottom-line takeaway: a fair assessment of Monza 2025 hinges on perspective.\n  - If you value strategic efficiency and team-point optimization, you may call it fair.\n  - If you value on-track equal opportunity and transparent governance, you may call it unfair or at least in need of clearer rules.\n  - The Torque Hunter piece reinforces that this is a debate about rules, communication, and the balance between team objectives and individual racing freedom."
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T16:52:43.213444Z",
     "start_time": "2025-10-11T16:52:39.819951Z"
    }
   },
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "f1_prompto = \"\"\"\n",
    "\n",
    "**ROLE:** You are the Team Principal of [Your Team Name]. The board of directors trusts you to make the final call on our lineup for next season.\n",
    "**OBJECTIVE:** Your mission is to select the optimal driver and race engineer pairing to maximize our points in the Constructors' Championship. Your decision must be logical, data-driven, and financially sound.\n",
    "**INPUT DATA:**\n",
    "**1. Driver Performance Records `{driver_database}`:**\n",
    "**2. Budgetary Constraints `{team_budget}`:**\n",
    "**YOUR TASK: DECISION & JUSTIFICATION**\n",
    "Based *only* on the input data provided above, complete the following steps to arrive at your final decision.\n",
    "**1.  Driver Shortlist Analysis:**\n",
    "    * From the `{driver_database}`, identify your top **two** potential drivers.\n",
    "    * For each driver, briefly justify your choice by analyzing their key performance metrics (e.g., qualifying pace, racecraft, consistency, tyre management) and how they align with our team's goals.\n",
    "**2.  Engineer Pairing & Synergy:**\n",
    "    * For each shortlisted driver, select the **best-suited race engineer** from the available engineers`.\n",
    "    * Explain *why* this pairing is optimal. Consider their communication styles, the driver's needs, and the engineer's technical expertise.\n",
    "**3.  Financial Review:**\n",
    "    * Review the combined salary expectations of your preferred pairing against our `{team_budget}`. Is this pairing financially viable?\n",
    "**4.  Final Recommendation:**\n",
    "    * State your final, decisive choice for the **one driver and one race engineer** who will join the team.\n",
    "    * Provide a concise, powerful summary of why this specific duo gives [Your Team Name] the greatest chance of success next season. If your top choice was too expensive, justify your selection of the more financially prudent option.\n",
    "    \"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(f1_prompto)\n",
    "client.push_prompt(\"f1teamprincipal-prompt\", object=french_prompt_template)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/f1teamprincipal-prompt/ed0f5f19?organizationId=079bb08e-1664-45cb-96ea-d507cd33eaaf'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T17:02:03.156682Z",
     "start_time": "2025-10-11T17:01:59.808300Z"
    }
   },
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence\", object=chain)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-runnable-sequence/29c42795?organizationId=079bb08e-1664-45cb-96ea-d507cd33eaaf'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
